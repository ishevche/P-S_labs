---
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

### Shevchenko Ivan, Burak Vasyl, Omelchuk Olesia


```{r}
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
```


```{r}
list.files(getwd())
list.files("data/0-authors")
```

```{r}
test_path <- "data/0-authors/test.csv"
train_path <- "data/0-authors/train.csv"

stop_words <- read_file("stop_words.txt")
# https://stackoverflow.com/questions/27195912/why-does-strsplit-return-a-list
splitted_stop_words <- strsplit(stop_words, split='\n')
splitted_stop_words <- splitted_stop_words[[1]]
```

## Classifier implementation

```{r}
naiveBayes <-
  setRefClass("naiveBayes",

              fields = list(data = "data.frame"),

              # Teaching model on train dataset
              methods = list(
                fit = function(X)
                {
                  tidy_words <- unnest_tokens(X, 'splitted', 'text', token = "words") %>% filter(!splitted %in% splitted_stop_words)
                  for (cur_author in unique(X$author)) {
                    df <- tidy_words %>%
                      filter(author == cur_author) %>%
                      count(splitted, name = cur_author, sort = TRUE)
                    df[cur_author] <- df[cur_author] + 1
                    df[nrow(df) + 1,] <- c("all", sum(df[, 2]))

                    data <<- merge(df, data, all.x = TRUE, all.y = TRUE, by = "splitted")
                  }
                  data_copy <- data[, -1]
                  rownames(data_copy) <- data[, 1]
                  data <<- data_copy
                  data[is.na(data)] <<- 1
                  data <<- data %>% mutate_at(colnames(data), as.numeric)
                  data["all"] <<- rowSums(data)
                },

                # Predict the author based on train set
                predict = function(message)
                {
                  col_amount <- ncol(data)
                  message_df <- data.frame(text = message)
                  words <- unnest_tokens(message_df, "word", "text")
                  probs <- data.frame()
                  probs["prob", seq_len(col_amount - 1)] <- (data["all", -col_amount] / data["all", "all"])["all",]
                  for (next_word in words[, 1]) {
                    if (!is.na(data[next_word, 1])) {
                      cur_word_num <- (data[next_word, -col_amount] / data["all", -col_amount])
                      probs["prob", seq_len(col_amount - 1)] <- probs["prob",] * cur_word_num
                    }
                  }
                  return(colnames(probs)[max.col(probs)])
                },

                # Calculate
                score = function(X_test)
                {
                  return(sum(lapply(X_test$text, function(x) return(predict(x))) == X_test$author) / nrow(X_test))
                },

                visualize_metrics = function(X_test) {
                  authors <- colnames(data[-ncol(data)])
                  ans <- data.frame()
                  ans[authors, authors] <- 0
                  for (i in seq_len(nrow(X_test))) {
                    pred <- predict(X_test[i, "text"])
                    actual <- X_test[i, "author"]
                    ans[pred, actual] <- ans[pred, actual] + 1
                  }
                  print(ans)
                  a <- data.frame()
                  for (author in authors) {
                    a[author, "precision"] <- ans[author, author] / sum(ans[, author])
                    a[author, "recall"] <- ans[author, author] / sum(ans[author,])
                    a[author, "f1"] <- 2 * a[author, "precision"] * a[author, "recall"] / (a[author, "recall"] + a[author, "precision"])
                  }
                  print(a)
                }
              ))

```

## Measure effectiveness of your classifier

```{r}
model <- naiveBayes$new(data = data.frame(splitted = "all"))


train <- read.csv(file = train_path, stringsAsFactors = FALSE)
model$fit(train)
test <- read.csv(file = test_path, stringsAsFactors = FALSE)
# Calculate precision, recall and F1-score
model$visualize_metrics(test[1:1000, ])
# Overall accuracy on test file
print("Accuracy:")
print(model$score(test))
```

```{r}
# show how accuracy depends on the size of train data
visualize_train_dependency <- function(step) {
  train_size <- seq(10, nrow(train), by = step)
  score <- NULL
  for (i in train_size) {
    test_model <- naiveBayes$new(data = data.frame(splitted = "all"))
    test_model$fit(train[1:i,])
    score <- c(score, test_model$score(test[1:500,]))
  }

  table <- data.frame(train_size, score)
  print(table)
  table %>%
    tail(10) %>%
    ggplot(aes(x = train_size, y = score)) +
    geom_line(color = "grey") +
    geom_point(shape = 21, color = "black", fill = "#69b3a2", size = 6) +
    ggtitle("Train")
}

visualize_train_dependency(1000)
```

## Conclusions
  To classify the piece of text with the author who was most likely to write it, we have to calculate the probabilities of that text being written by each author and take the maximum value. To calculate the probability of each author given that piece of text we can use Bayes’ Theorem. As the divisor will be the same for all authors, we can simply discard it. Then we calculate the probability of that text given each author and multiply it by the probability of that author. 
  While calculating the probability of that piece of text given the author, we split that text into words, get rid of “stop words”, don’t take into account the order of words, and simply multiply the probabilities of each word given that author (take the number of occurrences of that word in author’s texts and divide by the total number of words in author’s texts). When the piece of text that has to be classified is a word that doesn’t appear in texts of that author the probability of that word given that author is 0, and the total probability of that text becomes 0 as well. To solve this problem, we use “Laplace smoothing”: add 1 to every count of words, so it is never 0.
  The results are visualized by visualize_metrics and visualize_train_dependency functions. In the first one, we calculate precision, recall, and F1-score. In the second one, we show how the size of the train data influences the accuracy of our prediction (the bigger data, the greater accuracy).
