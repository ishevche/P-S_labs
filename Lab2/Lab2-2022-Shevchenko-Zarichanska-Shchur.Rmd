---
title: 'P&S-2022: Lab assignment 2'
author: "Shevchenko Ivan, Zarichanska Yelyzaveta, Shchur Oleksandr"
output:
  html_document:
    df_print: paged
---

## Work-breakdown structure

-   Task1 : Zarichanska Yelyzaveta
-   Task2 : Shchur Oleksandr
-   Task3 : Shevchenko Ivan

```{r}
id <- 26
set.seed(id)
```

## General comments and instructions

-   Complete solution will give you $\bf 4$ points (out of 100 total). Submission deadline is **23:59 of 06 November 2022**\
-   The report must be prepared as an *R notebook*; you must submit to **cms** both the source *R notebook* **and** the generated html file\
-   At the beginning of the notebook, provide a work-breakdown structure estimating efforts of each team member\
-   For each task, include
    -   problem formulation and discussion (what is a reasonable answer to discuss);\
    -   the corresponding $\mathbf{R}$ code with comments (usually it is just a couple of lines long);\
    -   the statistics obtained (like sample mean or anything else you use to complete the task) as well as histograms etc to illustrate your findings;\
    -   justification of your solution (e.g. refer to the corresponding theorems from probability theory);\
    -   conclusions (e.g. how reliable your answer is, does it agree with common sense expectations etc)\
-   The **team id number** referred to in tasks is the **two-digit** ordinal number of your team on the list. Include the line **set.seed(team id number)** at the beginning of your code to make your calculations reproducible. Also observe that the answers **do** depend on this number!\
-   Take into account that not complying with these instructions may result in point deduction regardless of whether or not your implementation is correct.

### Task 1

#### In this task, we discuss the $[7,4]$ Hamming code and investigate its reliability. That coding system can correct single errors in the transmission of $4$-bit messages and proceeds as follows:

-   given a message $\mathbf{m} = (a_1 a_2 a_3 a_4)$, we first encode it to a $7$-bit *codeword* $\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)$, where $G$ is a $4\times 7$ *generator* matrix\
-   the codeword $\mathbf{c}$ is transmitted, and $\mathbf{r}$ is the received message\
-   $\mathbf{r}$ is checked for errors by calculating the *syndrome vector* $\mathbf{z} := \mathbf{r} H$, for a $7 \times 3$ *parity-check* matrix $H$\
-   if a single error has occurred in $\mathbf{r}$, then the binary $\mathbf{z} = (z_1 z_2 z_3)$ identifies the wrong bit no. $z_1 + 2 z_2 + 4z_3$; thus $(0 0 0)$ shows there was no error (or more than one), while $(1 1 0 )$ means the third bit (or more than one) got corrupted\
-   if the error was identified, then we flip the corresponding bit in $\mathbf{r}$ to get the corrected $\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)$;\
-   the decoded message is then $\mathbf{m}^*:= (r_3r_5r_6r_7)$.

#### The **generator** matrix $G$ and the **parity-check** matrix $H$ are given by

$$  
    G := 
    \begin{pmatrix}
        1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 1 & 0 & 0 & 1 \\
    \end{pmatrix},
 \qquad 
    H^\top := \begin{pmatrix}
        1 & 0 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 & 1 & 1 \\
        0 & 0 & 0 & 1 & 1 & 1 & 1
    \end{pmatrix}
$$

#### Assume that each bit in the transmission $\mathbf{c} \mapsto \mathbf{r}$ gets corrupted independently of the others with probability $p = \mathtt{id}/100$, where $\mathtt{id}$ is your team number. Your task is the following one.

1.  Simulate the encoding-transmission-decoding process $N$ times and find the estimate $\hat p$ of the probability $p^*$ of correct transmission of a single message $\mathbf{m}$. Comment why, for large $N$, $\hat p$ is expected to be close to $p^*$.\
    \#### First, we set the **id** of the team and define the probability $p$ and the generator and parity-check matrices $G$ and $H$

    ```{r}
    # your team id number 
    id <-    26

    set.seed(id)
    p <- id/100
    # matrices G and H
    G <- matrix(c(1, 1, 1, 0, 0, 0, 0,
    		1, 0, 0, 1, 1, 0, 0,
    		0, 1, 0, 1, 0, 1, 0,
    		1, 1, 0, 1, 0, 0, 1), nrow = 4, byrow = TRUE)
    H <- t(matrix(c(1, 0, 1, 0, 1, 0, 1,
    		0, 1, 1, 0, 0, 1, 1,
    		0, 0, 0, 1, 1, 1, 1), nrow = 3, byrow = TRUE))

    #cat("The product GH must be zero: \n")
    #(G%*%H) %%2
    ```

    #### Now, we generate N massages and encode them to 7-bit codewords

    ```{r}
    N <- 1000
    message_generator <- function(N) {
      matrix(sample(c(0,1), 4*N, replace = TRUE), nrow = N)
    }  
    messages <- message_generator(N)
    codewords <- (messages %*% G) %% 2
    ```

    #### Here, we generate random errors with probability $p$

    ```{r}
    error_generator <- function(encoded_matrix) {
        output = matrix(, nrow = N, ncol = 7)
        for (i in 1:N) {
          for (j in 1:7) {
            error_probability = runif(1, min=0, max=1)
            output[i, j] = if (error_probability <= p) xor(encoded_matrix[i, j], 1)  else encoded_matrix[i, j]
          }
        }
        return (output)
    }
    received_with_errors <- error_generator(codewords)
    ```

    #### Using syndrome vector z we create a matrix with the indexes of bits that were wrongly transmitted

    ```{r}
    syndrome_vector <- (received_with_errors%*%H) %%2

    error_identifier <- function(z_matrix) {
      wrong_bits = matrix(, nrow = N, ncol = 1)
      for (i in 1:N) {
        wrong_bits[i,1] = z_matrix[i,1] + 2*z_matrix[i,2] + 4*z_matrix[i,3]
      }
      return (wrong_bits)
    }
    wrong_bits <- error_identifier(syndrome_vector)
    ```

    #### Next, we correct wrong bits and return decoded message

    ```{r}
    error_corrector <- function(wrong_bits) {
      corrected_message <- matrix(, nrow = N, ncol = 4)

      for (i in 1:N) {
        received_with_errors[i, wrong_bits[i]] = if (wrong_bits[i] != 0) xor(received_with_errors[i, wrong_bits[i]], 1) else received_with_errors[i, wrong_bits[i]]
        corrected_message[i, 1] = received_with_errors[i, 3]
        corrected_message[i, 2] = received_with_errors[i, 5]
        corrected_message[i, 3] = received_with_errors[i, 6]
        corrected_message[i, 4] = received_with_errors[i, 7]
      }
      return (corrected_message)
    }

    corrected_decoded_message <- error_corrector(wrong_bits)
    ```

    #### Finally, we find the estimate $\hat p$ of the probability $p^*$ of correct transmission of a single message $\mathbf{m}$

    ```{r}
    proper_transmissions <- 0
    for (i in 1:N) {
        a = (messages[i, 1:4] == corrected_decoded_message[i, 1:4])
        false_count = sum(a == 0)
        if (false_count == 0) {
          proper_transmissions = proper_transmissions + 1
        }
    }
    proper_transmissions/N
    ```

2.  By estimating the standard deviation of the corresponding indicator of success by the standard error of your sample and using the CLT, predict the \emph{confidence} interval $(p^*-\varepsilon, p^* + \varepsilon)$, in which the estimate $\hat p$ falls with probability at least $0.95$.\ What choice of $N$ guarantees that $\varepsilon \le 0.03$?\
We need to prove that: P{|p^ - p*|<= 0.03} >= 0.95

dev =  $\sqrt[]{p(1-p}$ 

Zn = ($\sqrt[]{n}$(p^ - p))/dev

Then we will find P{|p^ - p*|>= e}

A = P{|p^ - p|>= e} = P{|Zn|>=$\sqrt[]{n}$*e/$\sqrt[]{p(1-p}$} = 2P{Zn<=-$\sqrt[]{n}$*e/$\sqrt[]{p(1-p}$}

Using CLT, A ~ 2F(-$\sqrt[]{n}$*e/$\sqrt[]{p(1-p}$)

To make that be at most 0.05 with e <= 0.03 we need

$\sqrt[]{n}$*e/$\sqrt[]{p(1-p}$ >= 1.96

than we have

$\sqrt[]{n}$ >= 1.96/(2 * 0.03)

n >= 1067,1
4.  Draw the histogram of the number $k = 0,1,2,3,4$ of errors while transmitting a $4$-digit binary message. Do you think it is one of the known distributions?
    ```{r}
    num_of_errors_list <- matrix(, nrow = 1, ncol = N)
    for (row in 1:N) {
      num_of_errors_list[1, row] = sum(corrected_decoded_message[row,] != messages[row,])
    }
    hist(num_of_errors_list,
         xlim = c(0,4),
         xlab = "no of wrong bits",
         ylab = "no of  messages",
         main = "The num of wrong bits in a decoded message",
         col = "orange");
    ```

### Task 2.

#### In this task, we discuss a real-life process that is well modelled by a Poisson distribution. As you remember, a Poisson random variable describes occurrences of rare events, i.e., counts the number of successes in a large number of independent random experiments. One of the typical examples is the **radioactive decay** process.

#### Consider a sample of radioactive element of mass $m$, which has a big *half-life period* $T$; it is vitally important to know the probability that during a one second period, the number of nuclei decays will not exceed some critical level $k$. This probability can easily be estimated using the fact that, given the *activity* ${\lambda}$ of the element (i.e., the probability that exactly one nucleus decays in one second) and the number $N$ of atoms in the sample, the random number of decays within a second is well modelled by Poisson distribution with parameter $\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro constant, and $M$ is the molar (atomic) mass of the element. The activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is measured in seconds.

#### Assume that a medical laboratory receives $n$ samples of radioactive element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life period $T = 30.1$ years and mass $m = \mathtt{team\, id \,number} \times 10^{-6}$ g each. Denote by $X_1,X_2,\dots,X_n$ the **i.i.d. r.v.**'s counting the number of decays in sample $i$ in one second.

1.  Specify the parameter of the Poisson distribution of $X_i$ (you'll need the atomic mass of *Cesium-137*)\
2.  Show that the distribution of the sample means of $X_1,\dots,X_n$ gets very close to a normal one as $n$ becomes large and identify that normal distribution. To this end,
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical cumulative distribution function $\hat F_{\mathbf{s}}$ of $\mathbf{s}$;
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} $F$ of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $\hat F_{\mathbf{s}}$ and plot both **c.d.f.**'s on one graph to visualize their proximity (use the proper scales!);
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.\
3.  Calculate the largest possible value of $n$, for which the total number of decays in one second is less than $8 \times 10^8$ with probability at least $0.95$. To this end,
    -   obtain the theoretical bound on $n$ using Markov inequality, Chernoff bound and Central Limit Theorem, and compare the results;\
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sum $s=x_1 + \cdots +x_n$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    -   calculate the number of elements of the sample which are less than critical value ($8 \times 10^8$) and calculate the empirical probability; comment whether it is close to the desired level $0.95$

```{r}


setup <- function(id){
  N_avogadro <- 6.022 * 1e23
  Time <- 30.1 * 365 * 24 * 60 * 60 # half-life time in seconds
  m <- id * 1e-6 # mass of each sample in grams
  M <- 136.907089 # Atomic mass of Caesium-137.

  lambda <- log(2) / Time # The activity of the single element
  N <- (m * N_avogadro) / M # The amount of atoms in the sample
  mu <- N * lambda # Poisson distribution parameter
  return(mu)
}

samples_generation <- function(n, k, mu){
  set.seed(id)
  n <- n # samples quantity
  K <- k # Number of trials
  sample_means <- colMeans(matrix(rpois(n * K, lambda = mu), nrow = n)) # Generating samples and calculating their means.
  return(sample_means)
}

mu = setup(id)

n = 100
sample_means = samples_generation(n, 1e3, mu)

```

#### Next, calculate the parameters of the standard normal approximation

Let's denote as $S_n$ the sum of i.i.d.r.v.: $S_n$ = $X_1 + X_2 + ... + X_n$.

Then:

$E[S_n] = nE[X_i] = n \mu \\ Var[S_n] = n \sigma ^ 2$

$M_n = S_n/n \\ E[M_n] = E[S_n/n] = \mu \\ Var[M_n] = \sigma ^2 / n → 0$ Let's consider the standardized sums $Z_n=\frac {\sqrt{n}}{σ}(M_n−μ)= \frac{S_n−μn}{σ \sqrt{n}}.$

By the CLT, $Z_n$ converge in law to $Z∼N(0,1)$. $M_n = Z_n \frac {σ}{\sqrt{n}} + \mu$, therefore $M_n ∼ N(\mu, \frac {σ^2}{n})$.

As $X_i$ has the Poisson distribution, $Var[X_i] = \lambda = \sigma^2$. Hence $\sigma = \sqrt{\lambda}$

$\sigma_1 = \frac {σ}{\sqrt{n}} = \sqrt{\frac {\lambda}{n}}$

$\mu_1 = \mu$

$M_n ∼ N(\mu_1, \sigma_1^2)$

```{r}
sigma_1 <- sqrt(mu / n) # lambda is denoted as mu in this task
mu_1 <- mu
#mu_1
#sigma_1
```

#### We can now plot ecdf and cdf

```{r}
plot_cdf <- function(sample_means, n){
xlims <- c(mu_1 - 3 * sigma_1, mu_1 + 3 * sigma_1)
x <- seq(mu_1 - 3 * sigma_1, mu_1 + 3 * sigma_1, by = .001)
Fs <- ecdf(sample_means)
plot(Fs,
     xlim = xlims,
     ylim = c(0, 1),
     col = "blue",
     lwd = 2,
     main = paste("Comparison of ecdf and cdf, n = ", n, " repetitions = ", length(sample_means)))
curve(pnorm(x, mean = mu_1, sd = sigma_1), col = "red", lwd = 2, add = TRUE)

max_diff = max(abs(ecdf(sample_means)(x)-pnorm(x, mean = mu_1, sd = sigma_1)))
print(paste("Max difference is: ", max_diff))
}
for (x in c(5,10,50, 100)) {
  sigma_1 <- sqrt(mu / n) # lambda is denoted as mu in this task
  mu_1 <- mu
  sample_means = samples_generation(x, 1e3, mu)
  plot_cdf(sample_means, x)
}

```

#### Comments on the results

As CLT works for $n \ -> \infty$, we see that approximation becomes more precise as n increases.

#### Calculating bounds

$P(S_n < 8*10^8) \ge 0.95 \\ 1 - P(S_n \ge 8*10^8) \ge 0.95 \\ -P(S_n \ge 8*10^8) \ge -0.05 \\ P(S_n \ge 8*10^8) \le 0.05$ Using Markov: $P(S_n \ge 8*10^8) \le \frac{1}{8*10^8}*E[S_n] \le 0.05 \\ \frac{E[S_n]}{8*10^8} \le 0.05 \\ E[S_n] \le 4*10^7 \\ E[S_n] = n \mu \le 4*10^7 \\ n \mu \le 4*10^7 \\ n \le \frac{4*10^7}{\mu}$

```{r}
n_bound_Markov = 4*(10^7)/mu
print(n_bound_Markov)
```

Using Chebyshev:

For teamid = 26: $\mu = 83510392 \\ E[S_n] = n \mu \\ Var[S_n] = n \sigma ^ 2 \\ P(S_n \ge 8*10^8) \le 0.05 \\ P(S_n - n\mu \ge 8*10^8 - n\mu) \le P(\mid S_n - n\mu \mid \ge 8*10^8 - n\mu) \le 0.05 \\ P(\mid S_n - n\mu \mid \ge 8*10^8 - n\mu) = P(\mid M_n - \mu \mid \ge \frac{8*10^8}{n} - \mu ) \le 0.05$ Let's denote the $\frac{8*10^8}{n} - \mu = \frac{8*10^8 - n\mu}{n}$ as $c$ $\\ P(\mid M_n - \mu \mid \ge c) \le Var[S_n]/c^2 = n\sigma^2/c^2 = n\lambda/c^2 \le 0.05 \\ n\lambda/c^2 \le 0.05 \\ n^3 \lambda \le 0.05(8*10^8 - n\mu)^2 \\$As X has a Poisson distribution, \$\mu = \lambda \\ \$. $\\n^3 \lambda \le 0.05(8*10^8 - n\lambda)^2 \\ \lambda = 83510392$ Solving equation above leads to: $n \le 9.56517$

Using CLT:
$P(S_n \le 8*10^8 * (\sigma \sqrt{n}) + n*\mu) -> Ф(8*10^8)$

```{r}
bound_samples_generation <- function(n, k, mu){
  set.seed(id)
  n <- n # samples quantity
  K <- k # Number of trials
  sample_sums <- colSums(matrix(rpois(n * K, lambda = mu), nrow = n)) # Generating samples and calculating their means.
  return(sample_sums)
}

samples = bound_samples_generation(9, 1000, mu) # We can't set n = 9.56517 (result of solving the Chebishev inequality), so it is 9.
less_than_critical = sum(samples < 8*10^8)
empirical_probability = less_than_critical / length(samples)
print(paste("Empirical probability, that for n=9 sum is less than 8*10^8: ", empirical_probability))

samples = bound_samples_generation(10, 1000, mu) # Let's try with 10
less_than_critical = sum(samples < 8*10^8)
empirical_probability = less_than_critical / length(samples)
print(paste("Empirical probability, that for n=10 sum is less than 8*10^8: ", empirical_probability))

print("That proves that our estimation through the inequalities is quite precise!")
```

**Next, proceed with all the remaining steps**

**Do not forget to include several sentences summarizing your work and the conclusions you have made!**

### Task 3.

#### In this task, we use the Central Limit Theorem approximation for continuous random variables.

#### One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by $X_k$ the random time between the $(k-1)^{\mathrm{st}}$ and $k^{\mathrm{th}}$ click of the counter.

1.  Show that the distribution of the sample means of $X_1, X_2,\dots,X_n$ gets very close to a normal one (which one?) as $n$ becomes large. To this end,
    -   simulate the realizations $x_1,x_2,\dots,x_n$ of the \textbf{r.v.} $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the \emph{empirical cumulative distribution} function $F_{\mathbf{s}}$ of $\mathbf{s}$;\
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $F_{\mathbf{s}}$ of and plot both \textbf{c.d.f.}'s on one graph to visualize their proximity;\
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;\
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.
2.  The place can be considered safe when the number of clicks in one minute does not exceed $100$. It is known that the parameter $\nu$ of the resulting exponential distribution is proportional to the number $N$ of the radioactive samples, i.e., $\nu = \nu_1*N$, where $\nu_1$ is the parameter for one sample. Determine the maximal number of radioactive samples that can be stored in that place so that, with probability $0.95$, the place is identified as safe. To do this,
    -   express the event of interest in terms of the \textbf{r.v.} $S:= X_1 + \cdots + X_{100}$;\
    -   obtain the theoretical bounds on $N$ using the Markov inequality, Chernoff bound and Central Limit Theorem and compare the results;\
    -   with the predicted $N$ and thus $\nu$, simulate the realization $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum $S = X_1 + \cdots + X_{100}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the $100^{\mathrm{th}}$ click;\
    -   estimate the probability that the location is identified as safe and compare to the desired level $0.95$

The next block of code implements function containing everything needed in subtask #1:

- at first of all I initialize some important variables such as $\nu$ - the expected number of clicks in a minute and $K$ - amount of simulations
- then I generate a matrix $n \times K$ and calculate a mean for each of $K$ columns, and as a result I have a vector with length K of means
- using CLT one can show the distribution of means is approximately normal distribution (will be shown later), so I set up mean ($\mu$) and standard deviation ($\sigma$) for that normal distribution
- if user asked to draw a graph (settled second argument to TRUE) I plot empirical cdf of our means and corresponding normal distribution cdf using parameters settled before
- finally function returns the maximum difference between empirical cdf and normal distribution's cdf

Let's find $\mu$ and $\sigma$ for normal distribution:
$$ F(M_n \leqslant x) = F(\frac{X_1+X_2+..+X_n}{n}\leqslant x) =$$
$$= F(\frac{X_1+X_2+..+X_n - n\mu}{n}\leqslant x - \mu) =$$
$$= F(\frac{X_1+X_2+..+X_n - n\mu}{\sqrt n \cdotp \sigma}\leqslant \frac{(x - \mu)\cdotp \sqrt n}{\sigma})$$
From this point, using CLT I can say:
$$F(\frac{X_1+X_2+..+X_n - n\mu}{\sqrt n \cdotp \sigma}\leqslant \frac{(x - \mu)\cdotp \sqrt n}{\sigma}) \approx \Phi (\frac{(x - \mu)\cdotp \sqrt n}{\sigma})$$
where $\Phi$ is cdf for standard normal distribution. So our random variable $M_n \sim \mathcal{N}(\mu; \frac{\sigma}{\sqrt n})$
As soon as for exponential distribution $\mu = \sigma = \frac{1}{\lambda}$ and $= \frac{1}{\nu_1}$ in our case, I can set parameters for normal distribution

```{r}
simulate_sample <- function(n, graph = FALSE) {
  nu1 <- id + 10
  K <- 1e3
  sample_means <- colMeans(matrix(rexp(n * K, rate = nu1), nrow = n))

  mu <- 1 / nu1
  sigma <- 1 / (nu1 * sqrt(n))

  if (graph) {
    xlims <- c(mu - 3 * sigma, mu + 3 * sigma)
    Fs <- ecdf(sample_means)
    plot(Fs,
         xlim = xlims,
         col = "blue",
         xlab = "value",
         ylab = "P(mean < value)",
         lwd = 2,
         main = paste("Compparation of empitrical cdf of ", K, " X ", n, "sample with normal distrbution cdf"))
    curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
  }

  x <- seq(min(sample_means), max(sample_means), by = .0001)
  return(max(abs(ecdf(sample_means)(x) - pnorm(x, mean = mu, sd = sigma))))
}
```
```{r}
test <- function() {
  cat(paste("The max difference for n = 5: ", simulate_sample(5), "\n"))
  cat(paste("The max difference for n = 10: ", simulate_sample(10), "\n"))
  cat(paste("The max difference for n = 50: ", simulate_sample(50), "\n"))
  cat(paste("The max difference for n = 1000: ", simulate_sample(1000, TRUE), "\n"))
}
test()
```
From the results we can see that for bigger n the resulting distribution is more accurate, and the difference is smaller, and this is what WLLN stands for (obviously there could be some fluctuations, because of randomness, but overall picture is such).
```{r}
K <- 1e3
nu1 <- (id + 10)
```
```{r}
simulate_geiger <- function(n) {
  K <- 1e3
  nu1 <- (id + 10)
  sums <- colSums(matrix(rexp(100 * K, rate = nu1 * n), nrow = 100))
  return(length(sums[sums >= 1]) / K)
}
```
After defining all necessary functions and variables we can calculate N in 4 different ways:

- using Markov's inequality:
$$
P( S\geqslant 1) \leqslant E( S) , E( S) =100\cdotp E( X_{i}) =\frac{100}{\nu _{1} N}$$
$$0.95\leqslant P( S\geqslant 1)  \leqslant \frac{100}{\nu _{1} N}$$
$$\frac{100}{\nu _{1} N} \geqslant 0.95\Longrightarrow N\leqslant \frac{100}{\nu _{1} \cdotp 0.95}$$
- using Chebyshev's inequality:
$$
P( S-E( S) \geqslant 1-E( S)) \leqslant \frac{\sigma _{S}^{2}}{\sigma _{S}^{2} +( 1-E( S))^{2}} =\frac{100\sigma _{X}^{2}}{100\sigma _{X}^{2} +( 1-100 E( X))^{2}} =$$
$$=\frac{\frac{100}{N^{2} \nu _{1}^{2}}}{\frac{100}{N^{2} \nu _{1}^{2}} +\left( 1-\frac{100}{N\nu _{1}}\right)^{2}} =\frac{100}{100 + ( N\nu _{1} -100)^{2}} =\frac{100}{N^{2} \nu _{1}^{2} -200\cdotp N\nu _{1} +9000}$$
$$0.95\leqslant \frac{100}{N^{2} \nu _{1}^{2} -200\cdotp N\nu _{1} +9000}$$
$$0.95\left( 100 + ( N\nu _{1} -100)^{2}\right) =95+0.95( N\nu _{1} -100)^{2} \leqslant 100$$
$$0.95( N\nu _{1} -100)^{2} \leqslant 5$$
$$( N\nu _{1} -100)^{2} \leqslant \frac{5}{0.95}$$
$$N\leqslant \frac{\sqrt{\frac{20}{19}} +100}{\nu _{1}}$$

- using CLT approximation:
$$P\left(\frac{S-\mu n}{\sigma \sqrt{n}} \leqslant t\right)\rightarrow \Phi ( t)$$
$$P\left(\frac{S-\mu n}{\sigma \sqrt{n}} \geqslant t\right)\rightarrow 1-\Phi ( t)$$
$$P\left(\frac{S-\mu n}{\sigma \sqrt{n}} \geqslant \frac{1-\frac{100}{N\nu _{1}}}{\frac{10}{N\nu _{1}}}\right)\rightarrow 1-\Phi \left(\frac{1-\frac{100}{N\nu _{1}}}{\frac{10}{N\nu _{1}}}\right) =$$
$$=1-\Phi \left(\frac{N\nu _{1} -100}{10}\right) \geqslant 0.95\Longrightarrow \Phi \left(\frac{N\nu _{1} -100}{10}\right) \leqslant 0.05$$

- and also we can find n just simulating this process and saving largest $N$ where asked probability became less the 0.95
```{r}
N_markov <- 100 / (nu1 * 0.95)
N_chebashev <- (sqrt(5 / 0.95) + 100) / nu1
N_clt <- (uniroot(function(x) pnorm(x) - 0.05, c(-2, -1))$root * 10 + 100) / nu1
N_actual <- uniroot(function(x) simulate_geiger(x) - 0.95,
                    c(0.5, 5))$root
```
```{r}
test <- function() {
  cat(paste("The probability location identified as safe for n calculated by Markov\'s inequality (",
            N_markov, "): ", simulate_geiger(N_markov), "\n"))
  cat(paste("The probability location identified as safe for n calculated by Chebyshev\'s inequality (",
            N_chebashev, "): ", simulate_geiger(N_chebashev), "\n"))
  cat(paste("The probability location identified as safe for n calculated by CLT (",
            N_clt, "): ", simulate_geiger(N_clt), "\n"))
  cat(paste("The probability location identified as safe for n calculated by simulations (",
            N_actual, "): ", simulate_geiger(N_actual), "\n"))
}
test()
```

### General summary and conclusions

While doing this lab we have used such theorems and inequalities in real life and learned how to use then on practice. Particularly we have used Markov's, Chebyshev and Chernoff inequalities and also CLT alongside comparing with actual simulations of random events and also get better intuition about using such powerful instruments. Also, we have learned something new about real life random variables such as errors during data transmission and nuclear decay.